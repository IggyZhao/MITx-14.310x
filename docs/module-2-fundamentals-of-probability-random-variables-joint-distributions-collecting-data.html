<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</title>
  <meta name="description" content="Study notes taken from the courses for personal reference.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/MITx-14.310x/" />
  
  <meta property="og:description" content="Study notes taken from the courses for personal reference." />
  <meta name="github-repo" content="James-SR/MITx-14.310x" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)" />
  
  <meta name="twitter:description" content="Study notes taken from the courses for personal reference." />
  

<meta name="author" content="James Solomon-Rounce">


<meta name="date" content="2018-09-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="module-1-introduction-to-the-course.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html"><i class="fa fa-check"></i><b>1</b> Module 1: Introduction to the Course</a><ul>
<li class="chapter" data-level="1.1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html#introduction-to-r"><i class="fa fa-check"></i><b>1.1</b> Introduction to R</a><ul>
<li class="chapter" data-level="1.1.1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html#module-1-homework"><i class="fa fa-check"></i><b>1.1.1</b> Module 1 Homework</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><i class="fa fa-check"></i><b>2</b> Module 2: Fundamentals of Probability, Random Variables, Joint Distributions + Collecting Data</a><ul>
<li class="chapter" data-level="2.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#fundamentals-of-probability"><i class="fa fa-check"></i><b>2.1</b> Fundamentals of Probability</a><ul>
<li class="chapter" data-level="2.1.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#set-theory"><i class="fa fa-check"></i><b>2.1.1</b> Set Theory</a></li>
<li class="chapter" data-level="2.1.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#defining-probability"><i class="fa fa-check"></i><b>2.1.2</b> Defining Probability</a></li>
<li class="chapter" data-level="2.1.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#an-example"><i class="fa fa-check"></i><b>2.1.3</b> An example</a></li>
<li class="chapter" data-level="2.1.4" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#another-example"><i class="fa fa-check"></i><b>2.1.4</b> Another example</a></li>
<li class="chapter" data-level="2.1.5" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#ordered-and-unordered-arrangements"><i class="fa fa-check"></i><b>2.1.5</b> Ordered and Unordered Arrangements</a></li>
<li class="chapter" data-level="2.1.6" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#office-arrangements-and-pizza-toppings"><i class="fa fa-check"></i><b>2.1.6</b> Office Arrangements and Pizza Toppings</a></li>
<li class="chapter" data-level="2.1.7" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#independence-and-basketball-example"><i class="fa fa-check"></i><b>2.1.7</b> Independence and Basketball Example</a></li>
<li class="chapter" data-level="2.1.8" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#conditional-probability"><i class="fa fa-check"></i><b>2.1.8</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.1.9" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#conditional-probability-in-american-presidential-politics"><i class="fa fa-check"></i><b>2.1.9</b> Conditional Probability in American Presidential Politics</a></li>
<li class="chapter" data-level="2.1.10" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#bayes-theorem"><i class="fa fa-check"></i><b>2.1.10</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#random-variables-distributions-and-joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Random Variables, Distributions and Joint Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#probability-functions-of-random-variables"><i class="fa fa-check"></i><b>2.2.1</b> Probability Functions of Random Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#the-hypergeometric-distribution"><i class="fa fa-check"></i><b>2.2.2</b> The Hypergeometric Distribution</a></li>
<li class="chapter" data-level="2.2.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#steph-curry-shooting-example"><i class="fa fa-check"></i><b>2.2.3</b> Steph Curry Shooting example</a></li>
<li class="chapter" data-level="2.2.4" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#properties-of-the-probability-distribution"><i class="fa fa-check"></i><b>2.2.4</b> Properties of the Probability Distribution</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Module 2: Fundamentals of Probability, Random Variables, Joint Distributions + Collecting Data</h1>
<hr />
<p><strong>Module Sections:</strong></p>
<ul>
<li>Fundamentals of Probability</li>
<li>Random Variables, Distributions, and Joint Distributions</li>
<li>Gathering and Collecting Data</li>
<li>Module 2: Homework</li>
</ul>
<p>Module Content:</p>
<ul>
<li><a href="./files/M1/Lecture_Slides_02.pdf">Module 2 Slides - Fundamentals of Probability</a></li>
<li><a href="./files/M1/Lecture_Slides_03.pdf">Module 3 Slides - Random Variables, Distributions and Joint Distributions</a></li>
</ul>
<div id="fundamentals-of-probability" class="section level2">
<h2><span class="header-section-number">2.1</span> Fundamentals of Probability</h2>
<div id="set-theory" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Set Theory</h3>
<ul>
<li>A <em>sample space</em> is collection of all possible outcomes</li>
<li>An <em>event</em> is any collection of outcomes - could be one, all or none</li>
<li>If the outcome is a member of an event, the event is said to have <em>occured</em></li>
<li><p>Event B is said to be <em>contained</em> by A, if all outcomes in B also are in A</p></li>
<li>This is the basis of set theory and used widely in probability, although there are some differences between set and probability theory</li>
<li>If there is no symbol, then this usually means intersection AB in probability - in set theory we would write an inverted U e.g. <span class="math inline">\(A \cap B\)</span></li>
<li>A and B are mutually exclusive (disjoint in set theory) if they have no outcomes in common</li>
<li>A and B are exhaustive (complimentary in set theory) if their union is S (the entire sample space)</li>
<li><p>A and B are both mutually exclusive and exhaustive, their union is equal to the sample space but they have no events in common - they are a partition of the sample space</p></li>
</ul>
</div>
<div id="defining-probability" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Defining Probability</h3>
<p>We assign every event a number P(A) which is the prob. the event will occur</p>
<p>1 We require that the probability is greater than one for all events in the sample space - P(A) &gt;= 0 for all A c S 2 The entire sample space must be equal to one - P(S) = 1 3 For any sequence of disjoint sets, the prob. of the union of that sequence is equal to the sum of the probabilities of those events - A<sub>1</sub>, A<sub>2</sub>, … , is P(V<sub>i</sub>) = <span class="math inline">\(\sum_{i} P(A_i)\)</span></p>
<p>So we have a sample space, and if it satisfies these three properties, then we call it a probability. Sometimes this is referred to as a probability function or a probability distribution, but there is no standard terminology for all probability theory. Set theory helps to prove aspects of probability mathematically, for the purposes on this course, we just need to know what some useful facts are.</p>
<ul>
<li>P(A<sup>c</sup>) = 1 - P(A) =</li>
</ul>
<p>The probability of A compliment, which is the event that contains all of the outcomes that are not in the event A, the probability of A compliment is just equal to 1 minus the probability of A. This is useful if the probability of A comliment (P(A<sup>c</sup>)) is difficult to compute, where as the probability of A might be very easy to compute.</p>
<ul>
<li><span class="math inline">\(P (\emptyset)\)</span> =</li>
</ul>
<p>The probability of the empty set is zero.</p>
<ul>
<li>If A <sub>c</sub> B then P(A) &lt;= P(B) =</li>
</ul>
<p>If A is contained in B then the probability of A is less than or equal to the probability of B</p>
<ul>
<li>For all A, 0 &lt;= P(A) &lt;= 1 =</li>
</ul>
<p>For any events, the probability of that event is between 0 and 1.</p>
<ul>
<li>P(AUB) = P(A) + P(B) - P(AB) =</li>
</ul>
<p>Probability of A union B is just equal to the sum of the probabilities of those two events minus the probability of the their intersection.</p>
<ul>
<li>P(AB<sup>c</sup>) = P (A) - P(AB) =</li>
</ul>
<p>The probability of A times B complement is equal to the probability of A minus the probability of the intersection.</p>
</div>
<div id="an-example" class="section level3">
<h3><span class="header-section-number">2.1.3</span> An example</h3>
<p>Suppose you have a finite sample space. Let the function n(.) give the number of elements in a set.</p>
<p>Then define P(A) = n(A)/n(S). This is called a simple sample space, and it is a probability - we count the number of outcomes and divide by the number of possible outcomes in the sample space.</p>
<p>We can check that it satisfies the three axioms to ensure it is a probability:</p>
<ol style="list-style-type: decimal">
<li>P(A) will always be non-negative because it’s a count</li>
<li>P(S) will equal 1, by definition</li>
<li>P(AUB) = n(AUB)/n(S) = n(A)/n(S) + n(B)/n(S) = P(A) +P(B).</li>
</ol>
<p>If you can put your experiment in to this sample space where each outcome is equally likely, we just need to count to calculate probabilities of events. So for example, if you want to know how likely it is you will roll a specific number, say 6, on two dice, we calculate all the different ways that six occurs then divide this by all possible outcomes (sample space) - = 5 / 36 = or 13.9%</p>
</div>
<div id="another-example" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Another example</h3>
<p>If the state of Massachusetts issues 6-character license plates, using one of 26 letters and 10 digits randomly for each character, what is the probability that I will receive an all digit license plate?</p>
<p>n(S) = 36 (26 + 10) possibilities for each of 6 characters = 36<sup>6</sup> = 2.176b n(A) = 10 possibilities (for digits only) for each of 6 characters = 10<sup>6</sup> = 1m so P(A) = .0005</p>
<p>This is <em>sampling with replacement</em></p>
<p><em>What if Massachusetts does not reuse a letter or digit?</em></p>
<p>Now, in the sample space, there are 36 possibilities (26 + 10) for the 1st character, 35 left for the 2nd, and so on.</p>
<p>n(S) = 36x35x34x33x32x31 = 36!/30! = 1.402b</p>
<p>Similarly, in the event, there are 10 possibilities for the 1st character, 9 left for the 2nd, and so on.</p>
<p>n(A) = 10x9x8x7x6x5 = 10!/4! = 151k</p>
<p>so P(A) = 1.402b / 151k = .0001</p>
<p>This is <em>sampling without replacement</em></p>
</div>
<div id="ordered-and-unordered-arrangements" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Ordered and Unordered Arrangements</h3>
<p>In the examples so far, we have used a series of counting rules - combinatorics i.e. combinations of objects belonging to a finite set in accordance with certain constraints.</p>
<ol style="list-style-type: decimal">
<li><p>If an experiment has two parts, first one having m possibilities and, regardless of the outcome in the first part, the second one having n possibilities, then the experiment has m * n possible outcomes - this is what we do intuitively</p></li>
<li><p>Any ordered arrangement of objects is called a <em>permutation</em>. The number of different permutations of N objects is N! The number of different permutations of n objects taken from N objects is N!/(N-n)! This is the case in the license plate example previously given</p></li>
<li><p>Any unordered arrangement of objects is called a <em>combination</em>. The number of different combinations of n objects taken from N objects is N!/{(N-n)!n!}. We typically denote this <span class="math inline">\(\binom{N}{n}\)</span> - N (big objects) choose n (combinations). This is where the order of objects doesn’t matter i.e. different orderings don’t matter - we take out the ordering</p></li>
</ol>
<p>So if we had 9 people who each wanted to shake hands, if order doesn’t matter then it is a combination and we take 9 and choose 2 so it becomes:</p>
<p>9! / {(9-2)! * 2!} = 9! / {7! * 2!} = 362,880 / {5,040 * 2} = 362,880 / 10,080 = 36 combinations</p>
<p>Note, if order did matter and we used the permutations formula the total would be twice as many</p>
</div>
<div id="office-arrangements-and-pizza-toppings" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Office Arrangements and Pizza Toppings</h3>
<p>Q: If there are six vegetarian pizza toppings and five non-veg, if I randomly choose two from a hat containing all items, what is the probability that I end up with a pizza that has one veg and one non-veg topping?</p>
<p>A:</p>
<p>First we need to count the number of possabilities in the sample space e.g. {(V1, V2), (V1, V3), (V1, V4), (V1, N1) …} n(S) = <span class="math inline">\(\binom{11}{2}\)</span> = 55 - All outcomes are equally likely</p>
<p>Now we need to define our outcome n(A) = there are A = {(V1, N1), (V1, N2), (V2, N1) … } n(A) = 6 * 5 = 30</p>
<p>So the probability is N(A) / n(S) = 30 / 55 = 0.55</p>
<p>In general, I could have chose n toppings and asked what is the probability that my pizza had n1 vegetarian toppings and n2 non-vegetarian toppings. There would, then, be <span class="math inline">\(\binom{6}{n_1}\)</span> possibilities for the veg toppings and <span class="math inline">\(\binom{5}{n_2}\)</span>for the non-veg toppings. In other words,</p>
<p><span class="math display">\[P(n_1 veg, n_2  non-veg)=  \binom{6}{n_1} \binom{5}{n_2} \\ \binom{11}{n}\]</span> This is the basis of the hypergeometric distribution.</p>
</div>
<div id="independence-and-basketball-example" class="section level3">
<h3><span class="header-section-number">2.1.7</span> Independence and Basketball Example</h3>
<p>We call probabilistic events stochastic events. One of the most fundamental relationships between stochastic events is independence.</p>
<ul>
<li>Events A and B are independent if P(AB) = P(A) P(B)</li>
</ul>
<p>That is to say, events A and B are independent if the probability of their intersection is equal to the product of their probabilities.</p>
<ul>
<li>independent events is that knowing one event occurred doesn’t give you any information about whether the other occurred.</li>
</ul>
<p>This is best represented with an example. If you toss one die, once. Consider the event, A, that you roll a number less than 5, and the event, B, that you roll an even number. Are these events independent?</p>
<p>You might consider how could they be, as they rely on the same roll of a die?</p>
<p>If we use the previous example for independence, we check:</p>
<ol style="list-style-type: decimal">
<li><p>Probability of event A is P(A) = 2/3</p></li>
<li><p>Probability of event B is P(B) = 1/2</p></li>
<li><p>Probability of their intersection is P(AB) = 1/3 which is the same as P(A) P(B)</p></li>
</ol>
<p>So yes, it does satisfy the definition of independence. AB is rolling an even number less than 5 (e.g. 2 or 4) and P(A)P(B) = P(AB)</p>
<p><strong>So knowing one event occurred doesn’t give you any information about whether an other occured</strong></p>
<p>In another example, if we had a bag of ten poker chips numbered 1 to 10, with 3 different colours - <span class="math inline">\(\color{red}{\text{Red(1,2,3,4,5)}}\)</span>, <span class="math inline">\(\color{blue}{\text{Blue(6,7)}}\)</span> or <span class="math inline">\(\color{green}{\text{Green(8,9,10)}}\)</span></p>
<p>If choosing a poker chip, A that it is blue, and B that it is even, independent?</p>
<ol style="list-style-type: decimal">
<li><p>Probability of event A is P(A) = 2/10 (.2)</p></li>
<li><p>Probability of event B is P(B) = 5/10 (.5)</p></li>
<li><p>Probability of their intersection is P(AB) = 1/10 (or .1) which is the same as P(A) P(B)</p></li>
</ol>
<p>So yes they are independent, knowing one (that it is blue) does not give you any information about an other event (it is even).</p>
<p>Note that mutually exclusivity (disjoint events) and independence are not the same. Mutually exclusive events are not independent, and independent events cannot be mutually exclusive. Events are mutually exclusive if P (A and B) = 0.</p>
<p>So our independent events - blue and even - are not mutually exclusive, they can occur at the same time. Put another way, because events can’t happen at the same time (disjoint or mutually exclusive), they can’t be independent.</p>
<p>So if we take two mutually exclusive events - say the probability of a poker chip being both green (A) and blue (B) - we can check for the three parts of independence as:</p>
<ol style="list-style-type: decimal">
<li><p>Probability of event A is P(A) = 3/10 (.3)</p></li>
<li><p>Probability of event B is P(B) = 2/10 (.2)</p></li>
<li><p>Probability of their intersection is P(AB) = 0 which is not the same as P(A) P(B) (which is 0.06)</p></li>
</ol>
<p>As P (AB) = 0 i.e. they are mutually exclusive they are dependent - knowing one i.e. the chip blue DOES give you information about whether the other event occured - you know it is not green, so the probability of being green goes from 30% before being told, to 0% after being told it is blue.</p>
<p><strong>When events are mutually exclusive, when you know one thing is true the likelihood of the otehr being true becomes zero</strong></p>
<p>For more than two events, we define independence the same way - the events are independent if the probability of their intersection is equal to the product of their probabilities.</p>
</div>
<div id="conditional-probability" class="section level3">
<h3><span class="header-section-number">2.1.8</span> Conditional Probability</h3>
<p>What if knowing one event has occured tells us something about the probability that another event occured? How can we ‘update’ our knowledge in the event that the first event has occured?</p>
<p>The probability of A conditional on B is denoted as P(A|B). So the probability of A conditional on B, P(A|B), is P(AB)/ P(B), assuming P(B) &gt; 0. We don’t condition on an event if the probability of an event is 0%.</p>
<p>So in effect, by knowing one event has occured, it changes or re-defines our numerator for event B AND it is changing or re-defining our denominator - the part of the sample space which is now relevant - of event B.</p>
<p>There is a relationship between indepdence and conditional probability. Suppose A and B are independent and P(B) &gt; 0. Then,</p>
<p>P(A|B) = P(AB)/P(B) = P(A)P(B) (as they are indepdent this is the same as P(AB)) / P(B) = P(A) (we cancel out P(B) from the previous)</p>
<p>or simply</p>
<p>P(A|B) = P(AB)/P(B) = P(A)P(B)/P(B) = P(A)</p>
</div>
<div id="conditional-probability-in-american-presidential-politics" class="section level3">
<h3><span class="header-section-number">2.1.9</span> Conditional Probability in American Presidential Politics</h3>
<p>If candidates for Republican nomination had the following probabilities - these might be obtained from looking at betting markets</p>
<p>Trump P(A<sub>1</sub>) = .4<br />
Cruz P(A<sub>2</sub>) = .3<br />
Rubio P(A<sub>3</sub>) = .2<br />
Carson P(A<sub>4</sub>) = .1</p>
<p>How can we compute the probability of a Republican win for the presidency or P(W) i.e. the general election?</p>
<p>Conditional on winning the nomination, the candidates have following probabilities of winning the general election:</p>
<p>Trump P(W|A<sub>1</sub>) = .25<br />
Cruz P(W|A<sub>2</sub>) = .2<br />
Rubio P(W|A<sub>3</sub>) = .6<br />
Carson P(W|A<sub>4</sub>) = .4</p>
<p>The probability of a Republic win is equal to the probability of the intersection between a Republican win and the sample space.</p>
<p>The sample space is the union between the four events A1 through A4. A1 through A4 are mutually exclusive and exhaustive events and therefore form a partition.</p>
<p>In terms of notation, we therefore have:</p>
<p>P(W) = P(WS)</p>
<p>= P(W(A<sub>1</sub> U A<sub>2</sub> U A<sub>3</sub> U A<sub>4</sub>)) because A1-A4 are mutually exclusive and exhaustive sets, a partition<br />
= P(WA<sub>1</sub> U WA<sub>2</sub> U WA<sub>3</sub> U WA<sub>4</sub>)<br />
= P(WA<sub>1</sub>) + P(WA<sub>2</sub>) + P(WA<sub>3</sub>) + P(WA<sub>4</sub>)<br />
= P(W|A<sub>1</sub>)P(A<sub>1</sub>) + P(W|A<sub>2</sub>)P(A<sub>2</sub>) + P(W|A<sub>3</sub>)P(A<sub>3</sub>) + P(W|A<sub>4</sub>)P(A<sub>4</sub>)</p>
<p>So P(W) = .4x.25 + .3x.2 + .2x.6 + .1x.4 = .32</p>
</div>
<div id="bayes-theorem" class="section level3">
<h3><span class="header-section-number">2.1.10</span> Bayes’ Theorem</h3>
<p>So far, we have seen that the probability of the intersection between A and B is equal to the Probability of B conditional on A times the probability of A:</p>
<ul>
<li>P(AB) = P(B|A)P(A) = P(A|B)P(B)</li>
<li>provided P(A) &gt; 0 and P(B) &gt; 0 i.e. both A and B have positive probabilities</li>
<li>so we can write P(A|B) = P(B|A)P(A)/P(B)</li>
</ul>
<p>We also saw a slightly more complicated version of this, where the probability of B is the probability of B conditional on A times the probability of A, plus the probability of B conditional on A complement times the probability of A complement (note we saw this, albeit with more compliments, when looking at the Conditional Probability in American Presidential Politics section)</p>
<ul>
<li>P(B) = P(B|A)P(A) + P(B|Ac)P(Ac)</li>
<li>P(A|B) = P(B|A)P(A)/{P(B|A)P(A) + P(B|Ac)P(Ac)}</li>
</ul>
<p>C is compliment, and we can do this since A and Ac are partitions of the sample space S.</p>
<p>A pregnant woman lives in an area where the Zika virus is fairly rare - 1 in 1000 people have it. Still, she’s concerned, so she gets tested. There is a good but not perfect test for the virus—it gives a positive reading with probability .99 if the person has the virus and a positive reading with probability .05 if the person does not. Her reading is positive. How concerned should we be?</p>
<p>P(Z) = .001 (unconditional probability of having Zika) P(Zc) = .999 (999 people don’t have it) P(+|Z) = .99 (probability of having a positive test result, conditional on having the zika virus - there is a 1% change of a false negative) P(+|Zc) = .05 (probability of having a positive result if you don’t have the virus is 5% - false positive rate) P(Z|+) = P(+|Z)P(Z)/{P(+|Z)P(Z) + P(+|Zc)P(Zc)} - Bayes theorem = .019 - less than 2% probability</p>
<p>So the introduction of our new data results in us updating our probability based on the imperfect test, but it doesn’t get updated by much as it still possible it’s wrong and the prevelance rate of the zika virus is rare.</p>
<p><em>Example 2</em></p>
<p>Assume that the probability of having a rare condition is 1%. It is possible to test for the condition, but the test is imperfect. If you have the condition, there is an 85% chance that you will test positive. If you do not have the condition, there is a 5% chance that you will test positive. Call the condition C, so that P(C) = 0.01, and call a positive test t+, so that p(t+|C) = 0.85.</p>
<p>What is the probability p(t+) that you test positive for the condition?</p>
<p>So the Probability of having the condition is P(C) 0.01 * P(t+|C) = .85 which is the probability at a test you will test positive = 0.0085 + P(Cc) * P(t+|Cc) = 0.99 * 0.05 = 0.0495 = 0.058</p>
<p>Suppose that you tested positive for the condition. What is the probability that you truly have the underlying condition?</p>
<p>P(C) = .01 (unconditional probability of having condition) P(Cc) = .99 (99 people don’t have it) P(t+|C) = .85 (probability of having a positive test result, conditional on having the condition) P(t+|Cc) = .05 (probability of having a positive result if you don’t have the virus is 5% - false positive rate) P(C|+) = P(t+|C)P(C)/{P(t+|C)P(C) + P(t+|Cc)P(Cc)} - Bayes theorem = 0.0085 / {0.0085 + 0.0495} = .15 - around than 15% probability</p>
<p>Suppose that a new test is developed that is more accurate. Now, the probability of testing positive if you have the condition is 94%, and the chance of testing positive if you do not have the condition is only 4%. Now, what is the probability p(t+) that you test positive for the condition?</p>
<p>So the Probability of having the condition is P(C) 0.01 * P(t+|C) = .94 which is the probability at a test you will test positive = 0.0094 + P(Cc) * P(t+|Cc) = 0.99 * 0.04 = 0.0396 = 0.049</p>
<p>Suppose that you tested positive for the condition. What is the probability that you truly have the underlying condition?</p>
<p>P(C) = .01 (unconditional probability of having condition)<br />
P(Cc) = .99 (99 people don’t have it)<br />
P(t+|C) = .94 (probability of having a positive test result, conditional on having the condition)<br />
P(t+|Cc) = .04 (probability of having a positive result if you don’t have the virus is 5% - false positive rate)<br />
P(C|+) = P(t+|C)P(C)/{P(t+|C)P(C) + P(t+|Cc)P(Cc)} - Bayes theorem<br />
= 0.0094 / {0.0094 + 0.0396}<br />
= .19 - around than 15% probability</p>
<p>Suppose that there is an 80% chance you will be invited to a dinner party on a Friday or Saturday evening. In contrast, there is only a 50% chance that you will be invited to a dinner party on one of the other nights of the week. Suppose that you know that you’ve been invited to a dinner party tonight, but have forgotten which day of the week it is. Once you know that you’ve been invited to a dinner party, what is the chance that it is either Friday or Saturday? (Please round your answer to 2 decimal places. For example, if the correct answer is 0.6724, please input 0.67.)</p>
<p>Hint: Using the notation of Zika question, Let Z:= { Fri, Sat} and Z^c = { M,T,W,Th,Sun}. Let “+” denote invitation. You are given Pr(“+”| Z) = 0.8 and Pr(“+”| Z^c) = 0.5. We want to compute Pr( Z | “+”)</p>
<p>P(Z) = .286 (unconditional probability of it being Friday or Saturday)<br />
P(Zc) = .714 (the other 5 days of the week)<br />
P(+|Z) = .8<br />
P(+|Zc) = .5<br />
P(Z|+) = P(+|Z)P(Z)/{P(+|Z)P(Z) + P(+|Zc)P(Zc)} - Bayes theorem<br />
= .389 - around 40% probability</p>
</div>
</div>
<div id="random-variables-distributions-and-joint-distributions" class="section level2">
<h2><span class="header-section-number">2.2</span> Random Variables, Distributions and Joint Distributions</h2>
<p>A <em>random variable</em> is a real-valued function whose domain is the sample space - it goes from the sample space to the real line.</p>
<p>A probability goes from the set of all subsets of the sample space in to the unit interval e.g. [0,1] between zero and 1</p>
<p>A random variable goes from the sample space to the real line and it has some numerial charecteristics of the sample space we are interested in.</p>
<p>The probability that something exists induces a distribution of the random variable, they are not the same.</p>
<p>There are two types of random variable:</p>
<ul>
<li>Discrete - one that can take on only a - finite or infinite - countably number of values</li>
<li>Continous - a random variable that can take on any value in some interval, bounded or unbounded, of the real line</li>
</ul>
<p>Discrete random variables can be approximated using a continous random variable, so we typically just use continous. Most of the example we have seen so far in this section have dealt with discreet random variables.</p>
<div id="probability-functions-of-random-variables" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Probability Functions of Random Variables</h3>
<p>For discrete random variables, we often start with a verbal description, calculate probabilities for each value of the random variable, and then write down a function or draw a graph describing those probabilities for different values of the random variable. This is called a probability function (PF). We saw one of these before in the hypogeometric and binomial, when looking at the pizza toppings.</p>
<p>Note that:</p>
<ul>
<li>The term probability density function is used to draw attention to the fact that we are discussing a continuous random variable.<br />
</li>
<li>The term probability mass function is used to draw attention to the fact that we are discussing a discrete random variable.<br />
</li>
<li>The term probability function - or sometimes just the term “distribution” - is used when we are speaking in more general terms, when we’re discussing both “flavors” of probability function or the distinction between the two types of probability functions/random variables doesn’t matter.</li>
</ul>
<p>Hypergeometric (pizza topping) random variable:</p>
<p>1 Verbal description - Let X be the number of vegetarian toppings I get on my pizza if I draw the Area Four toppings randomly (without replacement)</p>
<p>2 Calculation - We can calculate the probability that X = 0, 1, 2, and so forth, up to the maximum of 6 or n, whichever is smaller, using the formula from last time. Six is the maximum number of veg toppings available, n is the number of toppings chosen at random. If there are 0 toppings of a particular type, the result will be undefined, so we adjust 0! to be defined as just 1. Also, to be consistent with notation for the random variable, n1 from before now becomes x and since we only have two options, n2 now becomes n - x</p>
<p><span class="math display">\[P(x  veg, n - x  non-veg)=  \binom{6}{x} \binom{5}{n - x} \\ \binom{11}{n}\]</span></p>
<p>3 If we then take an example, such as 3 veg toppings - n = 3 - we can calculate the probabilities for each n</p>
<p>P(X=0) = 6/99<br />
P(X=1) = 36/99<br />
P(X=2) = 45/99<br />
P(X=3) = 12/99</p>
<p>And we can represent the probability function graphically, with points (aka point mass) then add vertical lines under each point to the axis to make it easier to read e.g.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

veggie_choices =<span class="st"> </span><span class="dv">6</span>
meat_choices =<span class="st"> </span><span class="dv">5</span>
num_toppings =<span class="st"> </span><span class="dv">3</span>
veggie_received =<span class="st"> </span><span class="dv">0</span><span class="op">:</span>num_toppings
v =<span class="st"> </span><span class="kw">dhyper</span>(<span class="dt">x =</span> veggie_received, 
           <span class="dt">m =</span> veggie_choices, 
           <span class="dt">n =</span> meat_choices, 
           <span class="dt">k =</span> num_toppings)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(v)) {
  <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Probability of &quot;</span>, 
               i<span class="op">-</span><span class="dv">1</span>,
               <span class="st">&quot; veggie toppings is: &quot;</span>,
               <span class="kw">round</span>(v[i], <span class="dv">3</span>)))
}</code></pre></div>
<pre><code>## [1] &quot;Probability of 0 veggie toppings is: 0.061&quot;
## [1] &quot;Probability of 1 veggie toppings is: 0.364&quot;
## [1] &quot;Probability of 2 veggie toppings is: 0.455&quot;
## [1] &quot;Probability of 3 veggie toppings is: 0.121&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">y =</span> v)) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">geom_point</span>(<span class="dt">color =</span> <span class="st">&#39;red&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Num Veggies&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Probability&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">geom_segment</span>(<span class="dt">xend =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">yend=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Module2_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="the-hypergeometric-distribution" class="section level3">
<h3><span class="header-section-number">2.2.2</span> The Hypergeometric Distribution</h3>
<p>We can represent this in a more general way using notation. We say that X has a “hypergeometric distribution with parameters N, K, &amp; n,” denoted X ~ H(N,K,n). Where</p>
<ul>
<li>N = Total number of toppings</li>
<li>K = Total number of veg toppings</li>
<li>n = The number we choose</li>
</ul>
<p>Its Probability Function (PF) is defined similar to before, however we add a note for which values of x that there is positive probability. We should, if being fully formal, also add a final part which states it is 0 otherwise. If this is not explicit, as shown below, in terms of the zero otherwise, we can assume this to be the case.</p>
<p><span class="math display">\[fx(x)=  \binom{K}{x} \binom{N-K}{n - x} \\\binom{N}{n}\]</span> <span class="math display">\[ where \; x = max(0, n + K-N),...,min(n,K) \]</span></p>
<p>The hypergeometric distribution describes the number of number of “realized successes” (in a given sample - represented as x) in n trials where you’re sampling without replacement from a sample of size N, whose initial probability of success was K/N.</p>
<p>The function provides the probability of X (number of successful outcomes / number of possible outcomes in the sample space).</p>
</div>
<div id="steph-curry-shooting-example" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Steph Curry Shooting example</h3>
<p>If Steph has a probability of making 44% of any shot taken and therefore 56% chance of missing, we can use the binomial formula to calculate the probability of making n shots out of 6 possible shots as follows.</p>
<p>*<a href="https://en.wikipedia.org/wiki/Binomial_coefficient">For more information see the Binomial Coefficient</a></p>
<p>X has a “binomial distribution with parameters n &amp; p,” denoted <span class="math inline">\(X \sim B(n,p)\)</span>. Its PF is</p>
<p><span class="math display">\[fx(x)=  \binom{n}{x} p^x (1-p)^{n-x} \; \; \; where \; x= 0,1,...n\]</span></p>
<p>The binomial distribution describes the number of “successes” in n trials where the trials are independent and the probability of success in each is p.</p>
<p>So plugging in our example we get</p>
<p><span class="math display">\[fx(x)=  \binom{6}{x} .44^x (.56)^{6-x}\]</span></p>
<p>Which yields:</p>
<p>P(X=0) = .03<br />
P(X=1) = .15<br />
P(X=2) = .29<br />
P(X=3) = .30<br />
P(X=4) = .18<br />
P(X=5) = .06<br />
P(X=6) = .01</p>
<p>As the number of n increases, if p = 50% (a symetric distribution), the distribution would begin to look like a normal distribution.</p>
<p><img src="images/binomial.png" width="100%" /></p>
<p>In another example, suppose that you will take 3 penalty kicks in a row. The likelihood of making each penalty kick is ¾ or 75%. What is the probability that you will score 2 (and only 2) of the 3 penalty kicks?</p>
<p><span class="math display">\[fx(x)=  \binom{3}{x} .75^x (.25)^{3-x}\]</span> P(X=0) = .02<br />
P(X=1) = .14<br />
P(X=2) = .42 &lt;- this is the answer<br />
P(X=3) = .42</p>
</div>
<div id="properties-of-the-probability-distribution" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Properties of the Probability Distribution</h3>
<p>So for a general probability function, we have some broad properties:</p>
<ul>
<li><span class="math inline">\(0 &lt;= f_x(x_i) &lt;= 1\)</span> which is to say the value of any probability function is going to be between 0 and 1</li>
<li><span class="math inline">\(Σ_i f_x_ (x_i) = 1\)</span> if you sum up over all of the possible values it will sum to 1</li>
<li><span class="math inline">\(P(A) = P(XcA) = Σ_Af_x(x_i)\)</span> which is to say if you want the probability over a set of values of x, you just sum up the individual values for each item in the set</li>
</ul>
<p>For a continous random variable, we rarely start with a verbal description. Instead, we typically have a density that describes the probability that the random variable is in various regions. The density, or probability density function (PDF) is the continuous compliment to the discrete PF. The PF (discret) and PDF (continous) are similar but not exactly the same.</p>
<p>A random variable X is continuous if there exists a nonnegative function f_X_ such that for any interval A c R as follows. We tend to speak about a region, that A is in a region of the real line (R), the probability that X is in A is equal to the integral over that region A of the PDF.</p>
<p><span class="math display">\[P(X c A) = \int_{A} f_X(x)dx\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="module-1-introduction-to-the-course.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["bookdown-start.pdf", "bookdown-start.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
