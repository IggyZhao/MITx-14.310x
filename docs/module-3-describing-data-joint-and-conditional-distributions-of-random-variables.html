<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</title>
  <meta name="description" content="Study notes taken from the courses for personal reference.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/MITx-14.310x/" />
  
  <meta property="og:description" content="Study notes taken from the courses for personal reference." />
  <meta name="github-repo" content="James-SR/MITx-14.310x" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)" />
  
  <meta name="twitter:description" content="Study notes taken from the courses for personal reference." />
  

<meta name="author" content="James Solomon-Rounce">


<meta name="date" content="2018-10-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html"><i class="fa fa-check"></i><b>1</b> Module 1: Introduction to the Course</a><ul>
<li class="chapter" data-level="1.1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html#introduction-to-r"><i class="fa fa-check"></i><b>1.1</b> Introduction to R</a><ul>
<li class="chapter" data-level="1.1.1" data-path="module-1-introduction-to-the-course.html"><a href="module-1-introduction-to-the-course.html#module-1-homework"><i class="fa fa-check"></i><b>1.1.1</b> Module 1 Homework</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><i class="fa fa-check"></i><b>2</b> Module 2: Fundamentals of Probability, Random Variables, Joint Distributions + Collecting Data</a><ul>
<li class="chapter" data-level="2.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#fundamentals-of-probability"><i class="fa fa-check"></i><b>2.1</b> Fundamentals of Probability</a><ul>
<li class="chapter" data-level="2.1.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#set-theory"><i class="fa fa-check"></i><b>2.1.1</b> Set Theory</a></li>
<li class="chapter" data-level="2.1.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#defining-probability"><i class="fa fa-check"></i><b>2.1.2</b> Defining Probability</a></li>
<li class="chapter" data-level="2.1.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#an-example"><i class="fa fa-check"></i><b>2.1.3</b> An example</a></li>
<li class="chapter" data-level="2.1.4" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#another-example"><i class="fa fa-check"></i><b>2.1.4</b> Another example</a></li>
<li class="chapter" data-level="2.1.5" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#ordered-and-unordered-arrangements"><i class="fa fa-check"></i><b>2.1.5</b> Ordered and Unordered Arrangements</a></li>
<li class="chapter" data-level="2.1.6" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#office-arrangements-and-pizza-toppings"><i class="fa fa-check"></i><b>2.1.6</b> Office Arrangements and Pizza Toppings</a></li>
<li class="chapter" data-level="2.1.7" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#independence-and-basketball-example"><i class="fa fa-check"></i><b>2.1.7</b> Independence and Basketball Example</a></li>
<li class="chapter" data-level="2.1.8" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#conditional-probability"><i class="fa fa-check"></i><b>2.1.8</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.1.9" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#conditional-probability-in-american-presidential-politics"><i class="fa fa-check"></i><b>2.1.9</b> Conditional Probability in American Presidential Politics</a></li>
<li class="chapter" data-level="2.1.10" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#bayes-theorem"><i class="fa fa-check"></i><b>2.1.10</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#random-variables-distributions-and-joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Random Variables, Distributions and Joint Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#probability-functions-of-random-variables"><i class="fa fa-check"></i><b>2.2.1</b> Probability Functions of Random Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#the-hypergeometric-distribution"><i class="fa fa-check"></i><b>2.2.2</b> The Hypergeometric Distribution</a></li>
<li class="chapter" data-level="2.2.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#steph-curry-shooting-example"><i class="fa fa-check"></i><b>2.2.3</b> Steph Curry Shooting example</a></li>
<li class="chapter" data-level="2.2.4" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#properties-of-the-probability-distribution"><i class="fa fa-check"></i><b>2.2.4</b> Properties of the Probability Distribution</a></li>
<li class="chapter" data-level="2.2.5" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#discrete-versus-continuous-random-variables"><i class="fa fa-check"></i><b>2.2.5</b> Discrete versus Continuous Random Variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#a-note-on-terminology-and-the-uniform-distribution"><i class="fa fa-check"></i><b>2.2.6</b> A Note on Terminology and the Uniform Distribution</a></li>
<li class="chapter" data-level="2.2.7" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#the-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.2.7</b> The Cumulative Distribution Function</a></li>
<li class="chapter" data-level="2.2.8" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#joint-distributions"><i class="fa fa-check"></i><b>2.2.8</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.2.9" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#joint-distribution-example"><i class="fa fa-check"></i><b>2.2.9</b> Joint Distribution Example</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#gathering-and-collecting-data"><i class="fa fa-check"></i><b>2.3</b> Gathering and Collecting Data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#a-google-maps-api-example"><i class="fa fa-check"></i><b>2.3.1</b> A Google Maps API example</a></li>
<li class="chapter" data-level="2.3.2" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#web-scraping"><i class="fa fa-check"></i><b>2.3.2</b> Web Scraping</a></li>
<li class="chapter" data-level="2.3.3" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#human-research-subject-definitions"><i class="fa fa-check"></i><b>2.3.3</b> Human Research Subject Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html"><a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html#homework"><i class="fa fa-check"></i><b>2.4</b> Homework</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>3</b> Module 3: Describing Data, Joint and Conditional Distributions of Random Variables</a><ul>
<li class="chapter" data-level="3.1" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#summarizing-and-describing-data"><i class="fa fa-check"></i><b>3.1</b> Summarizing and Describing Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#kernel-density-estimation"><i class="fa fa-check"></i><b>3.1.1</b> Kernel Density Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#joint-marginal-and-conditional-probabilities"><i class="fa fa-check"></i><b>3.2</b> Joint, Marginal and Conditional Probabilities</a><ul>
<li class="chapter" data-level="3.2.1" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#marginal-or-individual-distributions"><i class="fa fa-check"></i><b>3.2.1</b> Marginal or Individual Distributions</a></li>
<li class="chapter" data-level="3.2.2" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#independence-of-random-variables"><i class="fa fa-check"></i><b>3.2.2</b> Independence of Random Variables</a></li>
<li class="chapter" data-level="3.2.3" data-path="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html"><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#conditional-distributions"><i class="fa fa-check"></i><b>3.2.3</b> Conditional Distributions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course notes from MITx 14.310x Data Analysis for Social Scientists (EdX)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="module-3-describing-data-joint-and-conditional-distributions-of-random-variables" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Module 3: Describing Data, Joint and Conditional Distributions of Random Variables</h1>
<hr />
<p><strong>Module Sections:</strong></p>
<ul>
<li>Summarizing and Describing Data</li>
<li>Joint, Marginal, and Conditional Distributions</li>
<li>R Tutorials: Basic Functions</li>
<li>Module 3: Homework</li>
</ul>
<p>Module Content:</p>
<ul>
<li><a href="./files/M3/SummarizingandDescribingDataSlides.pdf">Summarizing and Describing Data Slides</a></li>
<li><a href="module-3-describing-data-joint-and-conditional-distributions-of-random-variables.html#joint-marginal-and-conditional-probabilities">Joint, Marginal and Conditional Probabilities</a> (./files/M3/JointMarginalandConditionalProbabilities.pdf)</li>
</ul>
<div id="summarizing-and-describing-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Summarizing and Describing Data</h2>
<p>The goal of visualisation is either EDA for yourself or for conveying a message to other people. The course uses ggplot in both instances, this module focuses more on EDA for yourself.</p>
<p>One common way of initially looking at the data is to use a histogram, which provides a rough estimate of the probability distribution function (PDF) of a continous variable. We can have right open or closed sets when creating histogram bins <span class="math inline">\([a_i,b_i), [a_{i+1},b_{i+1})\)</span> see this video for a <a href="https://youtu.be/kREoWbByNZs">discussion on binning</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">require</span>(cowplot)</code></pre></div>
<pre><code>## Loading required package: cowplot</code></pre>
<pre><code>## 
## Attaching package: &#39;cowplot&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     ggsave</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)</code></pre></div>
<pre><code>## -- Attaching packages ------------------ tidyverse 1.2.1 --</code></pre>
<pre><code>## v tibble  1.4.2     v purrr   0.2.5
## v tidyr   0.8.1     v dplyr   0.7.6
## v readr   1.1.1     v stringr 1.3.1
## v tibble  1.4.2     v forcats 0.3.0</code></pre>
<pre><code>## -- Conflicts --------------------- tidyverse_conflicts() --
## x dplyr::filter()   masks stats::filter()
## x cowplot::ggsave() masks ggplot2::ggsave()
## x dplyr::lag()      masks stats::lag()</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bihar_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./files/M3/Bihar.csv&quot;</span>)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   personid = col_integer(),
##   female = col_integer(),
##   adult = col_integer(),
##   age = col_double(),
##   height_cm = col_double(),
##   weight_kg = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># keep the females</span>
bihar_adult_females &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(bihar_data, adult <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, female <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)

<span class="co"># take a look at our data</span>
<span class="kw">head</span>(bihar_adult_females, <span class="dv">10</span>)</code></pre></div>
<pre><code>## # A tibble: 10 x 6
##    personid female adult   age height_cm weight_kg
##       &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 11010103      1     1    28      150.      37.7
##  2 11010202      1     1    30      140.      57.3
##  3 11010207      1     1    35      148.      38.9
##  4 11010302      1     1    48      145.      35.7
##  5 11010303      1     1    22       NA       NA  
##  6 11010306      1     1    18       NA       NA  
##  7 11010308      1     1    28      145.      42.4
##  8 11010402      1     1    58      156.      51.1
##  9 11010404      1     1    36      156.      50.7
## 10 11010407      1     1    55      156.      47.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot it</span>
<span class="kw">ggplot</span>(bihar_adult_females, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>## Warning: Removed 1432 rows containing non-finite values (stat_bin).</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># not very attractive, so lets tidy it up - there are some outliers close to 0 and 200 cm</span>
bihar_adult_females_trunc &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(bihar_adult_females, height_cm <span class="op">&gt;</span><span class="st"> </span><span class="dv">120</span>, height_cm <span class="op">&lt;</span><span class="st"> </span><span class="dv">200</span>)

<span class="co"># Plot again with colour and labels</span>
<span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters, Bihar Females (truncated)&quot;</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<p>We could also adjust the bin width at this point if we wanted to. The width of the bins depends in part on the volume of data you have, for instance, if you have just 50 observations, picking bin widths of 1 cm might be too much - you can’t be sure your data is reliable, there may be quite a lot of noise. Conversely, if you have a million observations, 1 cm bin widths might be fine.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Bihar1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">binwidth =</span> <span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;binwidth = 5&quot;</span>)

Bihar2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">binwidth =</span> <span class="dv">10</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;binwidth = 10&quot;</span>)

Bihar3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">binwidth =</span> <span class="dv">20</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;binwidth = 20&quot;</span>)

Bihar4 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>, <span class="dt">binwidth =</span> <span class="dv">50</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;binwidth = 50&quot;</span>)

<span class="kw">plot_grid</span>(Bihar1, Bihar2, Bihar3, Bihar4, <span class="dt">labels=</span><span class="st">&quot;female height in Bihar&quot;</span>, <span class="dt">hjust =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">vjust =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="Module3_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># we could save the results to an image or a file using </span>
<span class="co"># ggsave(&quot;folder/bihargrid.pdf&quot;)</span></code></pre></div>
<div id="kernel-density-estimation" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Kernel Density Estimation</h3>
<p>To give a better and smoother representation of the data, we can use a kernel to visualise the data. It can be thought of as a smoothed histogram.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="kw">aes</span>(height_cm))</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In practice, it helps us to calculate our probability density function of the continous variable. As we previously saw, we cannot find the probability that a particular value of a continous variable is a particular value - it integrates to zero on a infinitley small scale. Intead, we can calculate the probability that it takes on some particular range of values. This is where the Kernel Density Estimation or KDE comes in.</p>
<p>We can draw a KDE with default parameters - a Gaussian distribution and auto bin width and no weights - using the density function. The autobin width are based on Silverman’s rule, which the R help notes</p>
<blockquote>
<p>bw.nrd0 implements a rule-of-thumb for choosing the bandwidth of a Gaussian kernel density estimator. It defaults to 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power (= Silverman’s ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31))) unless the quartiles coincide when a positive result will be guaranteed.</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">kde =<span class="st"> </span><span class="kw">density</span>(bihar_adult_females_trunc<span class="op">$</span>height_cm)
<span class="kw">plot</span>(kde)</code></pre></div>
<p><img src="Module3_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>If we were just calculating a density, then the mathmatical calulcation would be different than a KDE. KDE is like a weighted distribution, based on the individual points. A KDE has two paramers, K and h</p>
<ul>
<li>K = kernel aka the distribution</li>
<li>h = smoothing parameter aka bin width</li>
</ul>
<p>There are a number of different kernels, the Guaussian belongs to the un-bounded which means each event in the study region contributes to the estimated density at a specific location. This determines the overall shape of the curve around each of our observations x.</p>
<p>The smoothing parameter helps to determine at each particular point x, how much other observations around x contribute i.e. how much they are weighted. A higher bandwidth will result in a smoother curve, but may lead to the curve not fitting the underlying data well. The smoothing parameter also helps to determine the shape, by determining how far points contribute to the distribution’s peak - higher bandwidth results in points further away having an influence on the PDF and results in a flatter curve. A lower bandwidth conversely means that only points close to our particular observation x will play a part i.e. be weighted, so will result in a peaked curve around our particular x value.</p>
<p><span class="math display">\[\hat{f} ^{Kernel} (x) = \left. {\frac{1}{Nb} \sum_{i=1}^{N} K (\frac{x - x_i}{b})} \right.\]</span> * <a href="https://www.youtube.com/watch?v=gPWsDh59zdo">Kernel Density Estimation video</a> * <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">List of Kernels in Statistics</a></p>
<p>Two of the more common Kernels are the Epanechnikov and Normal distribution, with the former being bounded (see second link above).</p>
<ul>
<li>If the bandwidth is too wide, we will not fit the data well and introduce bias, the density plot will look flat and smooth</li>
<li>If the bandwidth is too narrow, we overfit our data, the density plot will look very peaked and jagged</li>
</ul>
<p>The goal is to be somewhere in the middle and to minimise the MSE. MSE can therefore be thought of as a measure or a quantification of the amount of the bias/variance tradeoff.</p>
<p>At the extremes, you will introduce some bias, as the kernel will stop at the boundary of the data, so it will tend to create a peak that is too high at the point - it it looking at only the data to the right at the left boundary and vice-versa. It is easier to see this in practice by looking at plots with higher (wide) bandwidths.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=zrEyxfl2-a8">Lecture on Bias-Variance Tradeoff, from Caltech EdX course</a></li>
</ul>
<p>Note - we can either use the stat_density function or geom_density - both are used in the following code as examples. I believe there are more options with stat_density.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Bihar5 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">bw =</span> <span class="dv">1</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;bandwidth = 1&quot;</span>)

Bihar6 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">bw =</span> <span class="dv">5</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;bandwidth = 5&quot;</span>)

Bihar7 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">bw =</span> <span class="dv">10</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;bandwidth = 10&quot;</span>)

Bihar8 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">bw =</span> <span class="dv">20</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;bandwidth = 20&quot;</span>)

<span class="kw">plot_grid</span>(Bihar5, Bihar6, Bihar7, Bihar8, <span class="dt">labels=</span><span class="st">&quot;female height in Bihar&quot;</span>, <span class="dt">hjust =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">vjust =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The kernel choice doesn’t affect the shape of the curve as much as the bandwith. The following four charts all have the same bandwidth, but different kernels. As previously noted, gaussian is unbounded meaning each point has some weight or contribution to the curve, where as the others are bounded meaning there is a limit as to which points contribute i.e. it has a finite interval. Rectangular and triangular are the two more unusual shapes compared to the other kernels available.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Bihar9 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;cosine&quot;</span>, <span class="dt">bw =</span> <span class="dv">20</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;kernel = cosine&quot;</span>)

Bihar10 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;epanechnikov&quot;</span>, <span class="dt">bw =</span> <span class="dv">20</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;kernel = epanechnikov&quot;</span>)

Bihar11 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">bw =</span> <span class="dv">20</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;kernel = rectangular&quot;</span>)

Bihar12 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm, ..density..), <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">kernel =</span> <span class="st">&quot;triangular&quot;</span>, <span class="dt">bw =</span> <span class="dv">20</span>, <span class="kw">aes</span>(height_cm), <span class="dt">fill =</span> <span class="ot">NA</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;kernel = triangular&quot;</span>)

<span class="kw">plot_grid</span>(Bihar9, Bihar10, Bihar11, Bihar12, <span class="dt">labels=</span><span class="st">&quot;Bandwidth 20, varying kernel&quot;</span>, <span class="dt">hjust =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">vjust =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-6-1.png" width="672" /> ### Comparing Distributions</p>
<p>The density we have seen so far is roughly bell shaped, it is uni-modal. The histogram almost looks like a binomial distribution, once the number of n for the binomial distribution became large enough. When comparing different distributions on the same chart, if the number of observations is different, it can be moe useful to plot the distributions as densities/proportions. For instance we can plot US adult height</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the US data </span>
<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(heights)

<span class="co"># keep the females,remove sex column and convert to cm</span>
US_adult_females &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(heights, sex<span class="op">==</span><span class="st">&#39;Female&#39;</span>)
US_adult_females &lt;-<span class="st"> </span>US_adult_females[<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>)]
<span class="kw">colnames</span>(US_adult_females)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;height_cm&quot;</span>
US_adult_females &lt;-<span class="st"> </span><span class="kw">cm</span>(US_adult_females)

<span class="co"># take a look at our data</span>
<span class="kw">head</span>(US_adult_females, <span class="dv">10</span>)</code></pre></div>
<pre><code>##    height_cm
## 1     165.10
## 2     167.64
## 3     157.48
## 4     167.64
## 5     162.56
## 6     152.40
## 7     162.56
## 8     170.18
## 9     167.64
## 10    170.18</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot it</span>
<span class="kw">ggplot</span>(US_adult_females, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lets tidy it up as before - there are some outliers close to 0 and 200 cm</span>
US_adult_females_trunc &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(US_adult_females, height_cm <span class="op">&gt;</span><span class="st"> </span><span class="dv">120</span>, height_cm <span class="op">&lt;</span><span class="st"> </span><span class="dv">200</span>)

<span class="co"># Plot again with colour and labels</span>
<span class="kw">ggplot</span>(US_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters, US Females (truncated)&quot;</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Let&#39;s plot both US and Bihar data on the same plot and tidy the code a little.  Note we don&#39;t need to aes() second time as the col is now the same name, so is applied to both since it is stated in the first line</span>

<span class="co"># Density plot for both</span>
<span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">data =</span> US_adult_females_trunc, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters&quot;</span>)</code></pre></div>
<p><img src="Module3_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Histogram for both</span>
<span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">data =</span> US_adult_females_trunc, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters&quot;</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Module3_files/figure-html/unnamed-chunk-7-4.png" width="672" /></p>
<p>We can clearly see in this instance the density plot is more appropriate, since the sample is much smaller (n=238) for the US data.</p>
<p>An alternative is to use the cumulative distribution function or CDF.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(bihar_adult_females_trunc, <span class="kw">aes</span>(height_cm)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_ecdf</span>(<span class="dt">color =</span> <span class="st">&quot;darkred&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_ecdf</span>(<span class="dt">data =</span> US_adult_females_trunc, <span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters&quot;</span>)</code></pre></div>
<p><img src="Module3_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Note: we could restructure the data at this point in to a single dataframe, so a legend appears. This is perhaps not the most elegant solution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># first we create a copy of our data with new column names, so they can be combined together and we can identify which is the releveant source</span>
USH &lt;-<span class="st"> </span>US_adult_females_trunc 
<span class="kw">colnames</span>(USH)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;US_Heights&quot;</span>
BiharH &lt;-<span class="st"> </span>bihar_adult_females_trunc[,<span class="dv">5</span>]
<span class="kw">colnames</span>(BiharH)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;Bihar_Heights&quot;</span>

<span class="co"># Next we gather the data i.e. go from wide to long then combine these in a single data frame by adding the rows</span>
USHG &lt;-<span class="st"> </span><span class="kw">gather</span>(USH)
BiharHG &lt;-<span class="st"> </span><span class="kw">gather</span>(BiharH)
combined &lt;-<span class="st"> </span><span class="kw">rbind</span>(USHG,BiharHG)

<span class="co"># Now we can create our plot</span>

<span class="kw">ggplot</span>(combined) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_ecdf</span>(<span class="kw">aes</span>(value, <span class="dt">colour =</span> key)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;darkblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Height in centimeters&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Cumulative Distribution&quot;</span>)</code></pre></div>
<p><img src="Module3_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>So we can see from the Cumulative Distribution Function that the curve below (US) is taller since at any point, there are fewer people - represented by the CDF - under that point. It has <em>First Order Stochastic Dominance or FOSD</em> - at any height in cm, there are fewer people under it in the US than India.</p>
<p>It is first order as it is based on shared preferences of possible outcomes and associated probabilities, not risk. FOSD is often considered in Game Theory and possible outcomes and decisions - statistically it is more likely and therefore better than another possible outcome and we don’t have to calculate utility. Risk aversion or apetite, this is considered under second order stochastic dominance or SOSD. We woulc consider or calculate utility in SOSD. Second order considerations come into play when thinking about outcomes that may have very similar or identical expected value e.g. an identical mean.</p>
<p>When comparing CDFs we look at First Order Stochastic Dominance i.e. which curve has it, we also the distance between the two lines and whether the pattern is the same, as it is above, throughout the curve or whether the lines cross and perhaps the dominance changes in a different region.</p>
<p>When comparing the US distribution of the heights vs the Indian one, the first-order stochastic dominance of the US distribution means that we are more likely to find larger height values in the US data.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=6iE_5y4r2FI">What is stochastic dominance</a></li>
</ul>
</div>
</div>
<div id="joint-marginal-and-conditional-probabilities" class="section level2">
<h2><span class="header-section-number">3.2</span> Joint, Marginal and Conditional Probabilities</h2>
<p>We previously ended the Math lecture from M1 on joint probabilities, we can look at another example. This time suppose we have a function <span class="math inline">\(F_{xy}(x,y)\)</span> where the function is $cx^2y ; for ; x^2 &lt;= y &lt;= 1 $ and is 0 otherwise and we want to calculate the probability for the joint PDF.</p>
<p>Note: c is the normalizing constant and ensures that our probability function as given becomes a PDF with a total probability of one i.e. it integrates to one.</p>
<p>First, we draw the support of this distribution i.e. where on the xy-plane there is positive probability. This is the element which is non-zero given above.</p>
<p><img src="images/jointsupport2D.png" width="100%" /></p>
<p>We can also show this as three dimensional.</p>
<p><img src="images/jointsupport3D.png" width="100%" /></p>
<p>Next, we integrate over this support area</p>
<p><span class="math display">\[∫_{-1}^{1}∫_{x^2}^{1}cx^2ydydx = \frac{4}{21} c\]</span></p>
<p>We can use the PRACMA package to calculate the value as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pracma)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;pracma&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) x<span class="op">^</span><span class="dv">2</span><span class="op">*</span>y
xmin &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">1</span>; xmax &lt;-<span class="st"> </span><span class="dv">1</span>
ymin &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span>; ymax &lt;-<span class="dv">1</span>
integral &lt;-<span class="st"> </span><span class="kw">integral2</span>(f, xmin,xmax,ymin,ymax)
integral<span class="op">$</span>Q </code></pre></div>
<pre><code>## [1] 0.1904762</code></pre>
<p>Which is the value of integral without c, but since we need c⋅integral without c = 1 we can find c simply by 1/ integral</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>integral<span class="op">$</span>Q </code></pre></div>
<pre><code>## [1] 5.25</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fractions</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>integral<span class="op">$</span>Q )</code></pre></div>
<pre><code>## [1] 21/4</code></pre>
<p>So c = 21 / 4 (which is 5.25). We can now use this value of c in our joint PDF formula</p>
<p><span class="math display">\[F_{xy}(x,y) = 5.25 x^2y \; for \; x^2 &lt;= y &lt;= 1 \; and \; 0 \; otherwise\]</span></p>
<p>Using this formula we can calculate proabilities of certain region, such as the probability that x &gt; y. To do so, we use the double integration over the region described by the probability statement e.g. x &gt; y. As before, we can draw the support region, then integrate over the area between the boundaries of the area (the lines) and the support. Using multivariable calculus, we set the limits of the integration then perform the integration. By substituting in our value of c we have:</p>
<p><span class="math display">\[∫_{1}^{0}∫_{x^2}^{x}5.25x^2ydydx = \frac{3}{20}\]</span> We can now calculate this as before</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="cf">function</span>(x,y,c) c <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>y
c &lt;-<span class="st"> </span><span class="fl">5.25</span>
xmin &lt;-<span class="st"> </span><span class="dv">1</span>; xmax &lt;-<span class="st"> </span><span class="dv">0</span>
ymin &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span>; ymax &lt;-<span class="cf">function</span> (x) x
I&lt;-<span class="st"> </span><span class="kw">integral2</span>(f, xmin,xmax,ymin,ymax, <span class="dt">c =</span> c) 
I<span class="op">$</span>Q </code></pre></div>
<pre><code>## [1] -0.15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>I<span class="op">$</span>Q</code></pre></div>
<pre><code>## [1] -6.666667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fractions</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>I<span class="op">$</span>Q) <span class="co"># as a fraction</span></code></pre></div>
<pre><code>## [1] -20/3</code></pre>
<div id="marginal-or-individual-distributions" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Marginal or Individual Distributions</h3>
<p>So far, we have looked at how to calculate regions of the xy plane, by integrating the PDF<span class="math inline">\(F_{xy}\)</span> over the regions. We can also use this joint PDF to recover individual or <em>marginal</em> distributions.</p>
<p>For discreet random variables for a particular value of x, we sum up the joint distribution over all the values for Y, to give the marginal distribution of x at that points. For continous variables we do the equivalent integration.</p>
<p>Discreet:</p>
<p><span class="math display">\[ f_X(x) = \sum_y f_{xy}(x,y) \]</span></p>
<p>Continous:</p>
<p><span class="math display">\[f_X(x) = \int_{y} f_{xy}(x,y)dy\]</span></p>
<p>This is best illistrated with an example. In tennis players tend to play at a similar level to the their opponent, if they are broadly at a similar level. So if one player is playing badly, so will the other. This suggests that the shape of joint PF of unforced errors in the game will be concentrated at points around which x and y have similar values, with less density (probability) around the points where one player has a lot more unforced errors than the other. The PF may look something like the one below.</p>
<p><img src="images/jointPFtennis.png" width="100%" /></p>
<p>To calculate the marginal/individual probabilities for one player, we just sum the probabilies - for each x, add up all the probabilities of y. We take a straight line at x and sum all the probabilities along that line/vector.</p>
<p>In our example, if we want to find the marginal PDF of a continous variable x, we simply integrate out y. Bearing in mind our conditions as before $cx^2y ; for ; x^2 &lt;= y &lt;= 1 $ and is 0 otherwise:</p>
<p><span class="math display">\[f_X(x) = \int_{x^2}^{1} 21/4 \; x^2ydy\]</span></p>
<p>If we wanted to find the marginal PDF of a y (a continous variable), we integrate out x in a similar fashion, again using the conditions as before.</p>
<p><span class="math display">\[f_y(y) = \int_{\sqrt{-y}}^{\sqrt{y}} 21/4 \; x^2ydx\]</span> More detail and charts are available in the slides at the top of this chapter.</p>
</div>
<div id="independence-of-random-variables" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Independence of Random Variables</h3>
<p>We have seen how to recover the marginal distributions from a joint distribution. Can we also construct the joint distribution from the marginal distribution? No we cannot, unless we know the relationship between the random variables. Joint distributions contain three pieces of information: how one variable is distributed, how a second variable is distributed, and the relationship between the two variables. Having the marginal distributions gives the first two pieces of information, but it does not give us information about how the two variables are related.</p>
<p>Independent random variables is often an assumption for some models and is a particular type of relationship between two variables.</p>
<p>X and Y are independent if the joint PDF is equal to the product of the marginal PDFs e.g. <span class="math inline">\(fxy(x,y)=fx(x)fy(y)\)</span></p>
<p>In our last example where the value of y takes on values depending on the value of x $cx^2y ; for ; x^2 &lt;= y &lt;= 1 $ so they are supported, so no they are not indepdent.</p>
<p>In the earlier of tennis unforced errors, our indepdendence definition is that x and y are independent if the probability that x is in some region and y is in some region is equal to the product of the probabilities for all regions a and b. There are regions where there is no probability (zero probability) in the joint probability function, yet the product of the two marginal distributions is non-zero. So this definition is not met.</p>
<p>For discrete random variables, if you have a table representing their joint probability function, the two variables are independent if and only if (iff) the rows of the table are proportional to one another, if and only if (iff) the columns of the table are proportional to one another - see below. The columns are simply ratios of the other columns.</p>
<p>If in the tennis example the unforced errors were indepdent, this is what the table of joint distributions would look like.</p>
<p><img src="images/indepdentrvs.png" width="100%" /></p>
</div>
<div id="conditional-distributions" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Conditional Distributions</h3>
<p>The conditional distribution allows us to “update” the distribution of a random variable, if we need to, given some information for instance y | x or number of unforced errors for y given x. Mathematically it is the joint PDF divided by marginal PDF:</p>
<p><span class="math display">\[FY|x (y|x) = fxy(x,y) / fx(x) \\ where \; P(Y=y |X=x) \; for X,Y discreet\]</span> In effect, it will behave like a marginal probability once we have conditioned (stated) on the x variable.</p>
<p>Note that we we condition (state) on one variable, the marginal probabilities at that point (slice) do not integrate to 1. So using the tennis example beofre, if one player has made 2 unforced errors (y = 2) so we are interested in the PDF of x at that point, we need to inflate the PDF by the probability that y = 2 or P(y = 2) which is 5/32 or 0.156 and we divide each of the individual probability to effectively gross-up the probabilities to 1.</p>
<p>For a continous conditional PDF, we normalise the marginal probability for one variable to effectively gross that up also.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="module-2-fundamentals-of-probability-random-variables-joint-distributions-collecting-data.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["bookdown-start.pdf", "bookdown-start.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
